{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run skopt/hyperopt hyper parameter sweep in neptune\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Training and evaluation script\n",
    "In order to run hyperparameter search you need to have a python script that runs model training and evaluation\n",
    "based on the parameters passed. \n",
    "\n",
    "We assume that you pass parameters via `ctx.params.YOUR_PARAM_NAME` and save the evaluation score in the `ctx.properties['YOUR_METRIC']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ctx = neptune.Context()\n",
    "\n",
    "data = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
    "                                                    test_size=0.2, random_state=1234)\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "params = {'boosting_type': ctx.params.boosting_type,\n",
    "          'objective': ctx.params.objective,\n",
    "          'num_class': ctx.params.num_class,\n",
    "          'num_leaves': ctx.params.num_leaves,\n",
    "          'max_depth': ctx.params.max_depth,\n",
    "          'learning_rate': ctx.params.learning_rate,\n",
    "          'feature_fraction': ctx.params.feature_fraction}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=ctx.params.num_boost_round,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                valid_names=['train', 'valid'],\n",
    "                )\n",
    "\n",
    "eval_loss= gbm.best_score['valid']['multi_logloss']\n",
    "ctx.properties['eval_loss'] = eval_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration file\n",
    "This is not strictly necessary but it makes things cleaner. \n",
    "It is a good idea to define hyperparameters and properties in a neptune configuration file.\n",
    "\n",
    "Let's call it `neptune.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project: neptune-ml/neptune-examples\n",
    "\n",
    "metric:\n",
    "  channel: 'eval_loss'\n",
    "  goal: minimize\n",
    "\n",
    "parameters:\n",
    "    boosting_type: 'gbdt'\n",
    "    objective: 'multiclass'\n",
    "    num_class: 3\n",
    "    num_boost_round: 10\n",
    "    learning_rate: 0.1\n",
    "    num_leaves: 10\n",
    "    max_depth: 10\n",
    "    feature_fraction: 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Optimize parameter sweep\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import skopt\n",
    "import neptunecontrib.hpo.utils as hp_utils\n",
    "import neptunecontrib.monitoring.skopt as sk_monitor\n",
    "\n",
    "ctx = neptune.Context()\n",
    "ctx.tags.append('skopt_forest_search')\n",
    "\n",
    "METRIC_CHANNEL_NAME = 'eval_loss'\n",
    "PROJECT_NAME = 'neptune-ml/neptune-examples'\n",
    "N_CALLS = 50\n",
    "N_RANDOM_STARTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space\n",
    "Scikit optimize search space is a list of skopt `Dimension` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [skopt.space.Integer(10, 60, name='num_leaves'),\n",
    "         skopt.space.Integer(2, 30, name='max_depth'),\n",
    "         skopt.space.Real(0.1, 0.9, name='feature_fraction', prior='uniform')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define objective function\n",
    "Every optimization algorithm needs an objective function to optimize.\n",
    "You can convert your `neptune run` command into such a function using `neptunecontrib.hpo.utils.make_objective` helper.\n",
    "Using the `@use_named_args(space)` decorator helps with the parameter formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@skopt.utils.use_named_args(space)\n",
    "def objective(**params):\n",
    "    return hp_utils.make_objective(params,\n",
    "                                   command=['neptune run --config neptune.yaml','train_evaluate.py'],\n",
    "                                   metric_channel_name=METRIC_CHANNEL_NAME,\n",
    "                                   project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NeptuneMonitor to observe metrics during training\n",
    "All you need to do in order to monitor the optimization process in neptune is add the `NeptuneMonitor` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = sk_monitor.NeptuneMonitor(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run skopt optimization\n",
    "Run the algorithm of your choice. I will use the `forest_minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = skopt.forest_minimize(objective, space, callback=[monitor],\n",
    "                                base_estimator='ET',\n",
    "                                n_calls=N_CALLS,\n",
    "                                n_random_starts=N_RANDOM_STARTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log best parameters and diagnostic charts to Neptune\n",
    "After the training has been finished you can log the best score and parameters and skopt diagnostic charts\n",
    "to neptune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "sk_monitor.send_best_parameters(results, ctx)\n",
    "sk_monitor.send_plot_convergence(results, ctx)\n",
    "sk_monitor.send_plot_evaluations(results, ctx)\n",
    "sk_monitor.send_plot_objective(results, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full scikit-optimize script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import skopt\n",
    "import neptunecontrib.hpo.utils as hp_utils\n",
    "import neptunecontrib.monitoring.skopt as sk_monitor\n",
    "\n",
    "ctx = neptune.Context()\n",
    "ctx.tags.append('skopt_forest_search')\n",
    "\n",
    "METRIC_CHANNEL_NAME = 'eval_loss'\n",
    "PROJECT_NAME = 'neptune-ml/neptune-examples'\n",
    "N_CALLS = 50\n",
    "N_RANDOM_STARTS = 10\n",
    "\n",
    "space = [skopt.space.Integer(10, 60, name='num_leaves'),\n",
    "         skopt.space.Integer(2, 30, name='max_depth'),\n",
    "         skopt.space.Real(0.1, 0.9, name='feature_fraction', prior='uniform')]\n",
    "\n",
    "@skopt.utils.use_named_args(space)\n",
    "def objective(**params):\n",
    "    return hp_utils.make_objective(params,\n",
    "                                   command=['neptune run --config neptune.yaml','train_evaluate.py'],\n",
    "                                   metric_channel_name=METRIC_CHANNEL_NAME,\n",
    "                                   project_name=PROJECT_NAME)\n",
    "\n",
    "monitor = sk_monitor.NeptuneMonitor(ctx)\n",
    "\n",
    "results = skopt.forest_minimize(objective, space, callback=[monitor],\n",
    "                                base_estimator='ET',\n",
    "                                n_calls=N_CALLS,\n",
    "                                n_random_starts=N_RANDOM_STARTS)\n",
    "\n",
    "ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "sk_monitor.send_best_parameters(results, ctx)\n",
    "sk_monitor.send_plot_convergence(results, ctx)\n",
    "sk_monitor.send_plot_evaluations(results, ctx)\n",
    "sk_monitor.send_plot_objective(results, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore optimization process in Neptune\n",
    "You can [explore the experiment here](https://app.neptune.ml/o/neptune-ml/org/neptune-examples/e/NEP-380/channels) if you want to.\n",
    "\n",
    "Check the training curve and hyperparameter logs.\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/0efc32299dbce7dd438e3b5811e933f0caa78504/contrib_hyper1.png)\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/0b94c0fc94f9e7a685ea89bc1c6307355b747253/contrib_hyper3.png)\n",
    "\n",
    "Explore hyperparameter diagnostics charts like `plot_objective` from the skopt package.\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/0b94c0fc94f9e7a685ea89bc1c6307355b747253/contrib_hyper4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt parameter sweep\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "import neptune\n",
    "import skopt\n",
    "from sklearn.externals import joblib\n",
    "import neptunecontrib.hpo.utils as hp_utils\n",
    "import neptunecontrib.monitoring.skopt as sk_monitor\n",
    "\n",
    "ctx = neptune.Context()\n",
    "ctx.tags.append('tpe_search')\n",
    "\n",
    "METRIC_CHANNEL_NAME = 'eval_loss'\n",
    "PROJECT_NAME = 'neptune-ml/neptune-examples'\n",
    "TRIALS_PATH = 'trials.pkl'\n",
    "N_CALLS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space\n",
    "Normally you define your search space in hyperopt by simply creating a dict.\n",
    "However, we want to make sure that the names are in the same order to be able\n",
    "to do some formatting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = OrderedDict(num_leaves=hp.choice('num_leaves', range(10, 60, 1)),\n",
    "                    max_depth=hp.choice('max_depth', range(2, 30, 1)),\n",
    "                    feature_fraction=hp.uniform('feature_fraction', 0.1, 0.9)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define objective function\n",
    "Every optimization algorithm needs an objective function to optimize. You can convert your neptune run command into such a function using neptunecontrib.hpo.utils.make_objective helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    return hp_utils.make_objective(params,\n",
    "                                   command=['neptune run --config neptune.yaml','train_evaluate.py'],\n",
    "                                   metric_channel_name=METRIC_CHANNEL_NAME,\n",
    "                                   project_name=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run hyperopt optimization\n",
    "Run the TPE algoright from via `fmin` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "_ = fmin(objective, space, trials=trials, algo=tpe.suggest, max_evals=N_CALLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log best parameters and diagnostic charts to Neptune\n",
    "After the training has been finished you can log the best score and parameters and skopt diagnostic charts\n",
    "to neptune. \n",
    "\n",
    "You need to convert `hyperopt.Trials` object into `scipy.optimize.OptimizeResult`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = hp_utils.hyperopt2skopt(trials, space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send parameters and diagnostic charts to neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "sk_monitor.send_runs(results, ctx)\n",
    "sk_monitor.send_best_parameters(results, ctx)\n",
    "sk_monitor.send_plot_convergence(results, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore optimization process in Neptune\n",
    "You can [explore the experiment here](https://app.neptune.ml/o/neptune-ml/org/neptune-examples/e/NEP-431/channels) if you want to.\n",
    "\n",
    "Check the training curve and hyperparameter logs.\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/3ec8efcf705507ddbe8aac709130460b451c2868/contrib_hyper21.png)\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/3ec8efcf705507ddbe8aac709130460b451c2868/contrib_hyper22.png)\n",
    "\n",
    "Explore hyperparameter diagnostics charts.\n",
    "\n",
    "![image](https://gist.githubusercontent.com/jakubczakon/f754769a39ea6b8fa9728ede49b9165c/raw/3ec8efcf705507ddbe8aac709130460b451c2868/contrib_hyper23.png)\n",
    "\n",
    "**Note**\n",
    "There is no callbacks in hyperopt only the trials object.\n",
    "It means that all the scores/hyperparamters are logged to neptune after the training has been finished.\n",
    "In order not to loose all that information if the process fails mid-way through I would suggest putting\n",
    "the `fmin` function into a `try catch` block:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    trials = Trials()\n",
    "    _ = fmin(objective, space, trials=trials, algo=tpe.suggest, max_evals=N_CALLS)\n",
    "    joblib.dump(trials, TRIALS_PATH)\n",
    "    \n",
    "    results = hp_utils.hyperopt2skopt(trials, space)\n",
    "\n",
    "    ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "    sk_monitor.send_runs(results, ctx)\n",
    "    sk_monitor.send_best_parameters(results, ctx)\n",
    "    sk_monitor.send_plot_convergence(results, ctx)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Process failed saving current trials to {}'.format(TRIALS_PATH))\n",
    "    joblib.dump(trials, TRIALS_PATH)\n",
    "    \n",
    "    results = hp_utils.hyperopt2skopt(trials, space)\n",
    "\n",
    "    ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "    sk_monitor.send_runs(results, ctx)\n",
    "    sk_monitor.send_best_parameters(results, ctx)\n",
    "    sk_monitor.send_plot_convergence(results, ctx)\n",
    "    raise(e)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full hyperopt script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import skopt\n",
    "from sklearn.externals import joblib\n",
    "import neptunecontrib.hpo.utils as hp_utils\n",
    "import neptunecontrib.monitoring.skopt as sk_monitor\n",
    "\n",
    "ctx = neptune.Context()\n",
    "ctx.tags.append('tpe_search')\n",
    "\n",
    "METRIC_CHANNEL_NAME = 'eval_loss'\n",
    "PROJECT_NAME = 'neptune-ml/neptune-examples'\n",
    "TRIALS_PATH = 'trials.pkl'\n",
    "N_CALLS = 50\n",
    "\n",
    "space = OrderedDict(num_leaves=hp.choice('num_leaves', range(10, 60, 1)),\n",
    "                    max_depth=hp.choice('max_depth', range(2, 30, 1)),\n",
    "                    feature_fraction=hp.uniform('feature_fraction', 0.1, 0.9)\n",
    "                   )\n",
    "\n",
    "def objective(params):\n",
    "    return hp_utils.make_objective(params,\n",
    "                                   command=['neptune run --config neptune.yaml','train_evaluate.py'],\n",
    "                                   metric_channel_name=METRIC_CHANNEL_NAME,\n",
    "                                   project_name=PROJECT_NAME)\n",
    "\n",
    "trials = Trials()\n",
    "_ = fmin(objective, space, trials=trials, algo=tpe.suggest, max_evals=N_CALLS)\n",
    "\n",
    "results = hp_utils.hyperopt2skopt(trials, space)\n",
    "\n",
    "ctx.channel_send(METRIC_CHANNEL_NAME, results.fun)\n",
    "sk_monitor.send_runs(results, ctx)\n",
    "sk_monitor.send_best_parameters(results, ctx)\n",
    "sk_monitor.send_plot_convergence(results, ctx)\n",
    "sk_monitor.send_plot_evaluations(results, ctx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
